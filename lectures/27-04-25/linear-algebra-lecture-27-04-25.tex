\documentclass[11pt, letterpaper]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{lmodern} % Or another font package for better aesthetics
\usepackage[T1]{fontenc}
\usepackage{mathtools} % For \coloneqq

% --- Theorem Environments ---
\newtheoremstyle{mytheoremstyle}
  {\topsep} % Space above
  {\topsep} % Space below
  {\itshape} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {} % Theorem head spec (can be left empty, meaning `normal')

\newtheoremstyle{mydefinitionstyle}
  {\topsep} % Space above
  {\topsep} % Space below
  {\normalfont} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {} % Theorem head spec

\newtheoremstyle{myremarkstyle}
  {\topsep} % Space above
  {\topsep} % Space below
  {\normalfont} % Body font
  {} % Indent amount
  {\itshape} % Theorem head font
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {} % Theorem head spec

\theoremstyle{mytheoremstyle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{mydefinitionstyle}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{myremarkstyle}
\newtheorem{remark}[theorem]{Remark}

% --- Custom Commands ---
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\V}{\mathcal{V}} % Using calligraphic V for vector spaces
\newcommand{\U}{\mathcal{U}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\ket}[1]{\left|#1\right\rangle} % Example for physics notation if needed
\DeclareMathOperator{\im}{Im} % Image of a map
\DeclareMathOperator{\spanv}{span} % Span
\DeclareMathOperator{\dimv}{dim} % Dimension
\newcommand{\mat}[1]{\begin{pmatrix}#1\end{pmatrix}} % Matrix shortcut
\newcommand{\coord}[2]{[#1]_{#2}} % Coordinate vector notation

% --- Document Information ---
\title{Lecture Notes: Linear Transformations, Isomorphisms, and Coordinates}
\author{Undergraduate Mathematics Educator} % Placeholder
\date{\today} % Or specific date

\begin{document}
\maketitle

% --- Administrative Notes Section ---
\section*{Announcements and Administrative Notes}
\begin{itemize}
    \item \textbf{Tutorial/Recitation Time Clarification:} There seems to be some confusion regarding the timing of tomorrow's session. Some mentioned 3:00 PM, others 4:30 PM. Please \textbf{check the official course schedule or yearbook} for the definitive time and location. We need to ensure everyone has the correct information.
    \item \textbf{Homework Exercises:} Keep an eye out for the upcoming problem set. Based on today's lecture, you can expect exercises related to:
        \begin{itemize}
            \item Proving properties of isomorphisms (e.g., the reverse direction of the theorems on preserving linear independence and spanning sets).
            \item Proving that the inverse of an isomorphism is itself a linear transformation (and thus an isomorphism).
            \item Finding bases for specific vector spaces, such as spaces of matrices, and determining their dimensions.
            \item Working with coordinate vectors relative to different bases.
        \end{itemize}
    \item \textbf{Lecture Break:} As mentioned during the lecture, we took a short break. Breaks are important for staying focused during longer sessions!
\end{itemize}
\hrulefill % Visual separator
\vspace{1em}

% --- Mathematical Content ---

\section{Introduction: Bridging the Abstract and Concrete}

Hello everyone. Today, we delve deeper into the structure of vector spaces. We've explored abstract vector spaces, which can be collections of polynomials, matrices, functions, or the familiar geometric vectors. A recurring theme in mathematics is finding connections between different structures. Our goal today is to understand how, under certain conditions, even the most abstract-seeming vector space can be viewed as being "essentially the same" as the very concrete space $\F^n$ (like $\R^n$). The key concept linking these spaces will be the idea of an \emph{isomorphism}, built upon the foundation of \emph{linear transformations}. We will discover that for finite-dimensional vector spaces, this connection is remarkably strong, allowing us to translate problems from an abstract setting into the comfortable realm of column vectors and matrices.

\section{Functions, Mappings, Transformations: The Basics}

Before specializing, let's clarify the general language of functions. In mathematics, the terms "function," "mapping," and "transformation" are often used interchangeably, especially in the context of vector spaces.

\begin{definition}[Function/Mapping/Transformation]
Let $X$ and $Y$ be any two sets. A \emph{function} (or \emph{mapping}, or \emph{transformation}) $f$ from $X$ to $Y$, denoted $f: X \to Y$, is a rule that assigns to \emph{each} element $x \in X$ \emph{exactly one} element $y \in Y$.
\begin{itemize}
    \item The set $X$ is called the \emph{domain} of $f$.
    \item The set $Y$ is called the \emph{codomain} of $f$.
    \item For an element $x \in X$, the unique element $y \in Y$ assigned by $f$ is denoted $f(x)$ and is called the \emph{image} of $x$ under $f$.
    \item The set of all images, $\{f(x) \mid x \in X\}$, is a subset of $Y$ called the \emph{image} or \emph{range} of $f$, often denoted $\im(f)$ or $f(X)$.
\end{itemize}
It's crucial that every element in the domain $X$ is mapped somewhere, and that no element in $X$ is mapped to more than one element in $Y$.
\end{definition}

\begin{remark}
Visually, if we represent sets as blobs and function application as arrows, the definition means:
\begin{itemize}
    \item Every point in $X$ must have exactly one arrow originating from it.
    \item It's okay for multiple arrows from $X$ to point to the same element in $Y$.
    \item It's okay for some elements in $Y$ to have no arrows pointing to them.
    \item It's *not* okay for a point in $X$ to have two or more arrows starting from it, pointing to different elements in $Y$.
\end{itemize}
\end{remark}

Two fundamental properties characterize how functions behave regarding distinctness of inputs and coverage of the codomain:

\begin{definition}[One-to-one / Injective]
A function $f: X \to Y$ is \emph{one-to-one} (or \emph{injective}) if distinct elements in the domain map to distinct elements in the codomain. Formally, for all $x_1, x_2 \in X$,
\[ \text{if } f(x_1) = f(x_2), \text{ then } x_1 = x_2. \]
Equivalently, if $x_1 \neq x_2$, then $f(x_1) \neq f(x_2)$.
\end{definition}

\begin{definition}[Onto / Surjective]
A function $f: X \to Y$ is \emph{onto} (or \emph{surjective}) if every element in the codomain $Y$ is the image of \emph{at least one} element from the domain $X$. Formally, for every $y \in Y$, there exists at least one $x \in X$ such that $f(x) = y$.
Equivalently, the image of $f$ is equal to the entire codomain: $\im(f) = Y$.
\end{definition}

\begin{example}[Basic Functions from $\R$ to $\R$]
Let's revisit familiar functions from calculus:
\begin{enumerate}
    \item $f: \R \to \R$ defined by $f(x) = x$.
        \begin{itemize}
            \item Is $f$ one-to-one? Yes. If $f(x_1) = f(x_2)$, then $x_1 = x_2$.
            \item Is $f$ onto? Yes. For any $y \in \R$ (codomain), we can choose $x = y \in \R$ (domain), and $f(x) = f(y) = y$.
        \end{itemize}
    \item $g: \R \to \R$ defined by $g(x) = x^2$.
        \begin{itemize}
            \item Is $g$ one-to-one? No. For example, $g(1) = 1^2 = 1$ and $g(-1) = (-1)^2 = 1$, but $1 \neq -1$. Distinct inputs map to the same output.
            \item Is $g$ onto? No. The image of $g$ is $[0, \infty)$, which is not the entire codomain $\R$. For instance, there is no $x \in \R$ such that $g(x) = x^2 = -1$. Negative numbers in the codomain are not mapped onto.
        \end{itemize}
\end{enumerate}
\end{example}

A function that is both one-to-one and onto is called \emph{bijective}. Bijections establish a perfect pairing between the elements of the domain and the codomain.

\section{Linear Transformations: Structure Preservation}

Now we focus on functions between vector spaces that respect the underlying algebraic structure (vector addition and scalar multiplication).

\begin{definition}[Linear Transformation]
Let $U$ and $V$ be vector spaces over the same field $\F$. A function $T: U \to V$ is called a \emph{linear transformation} (or linear map) if it satisfies the following two conditions:
\begin{enumerate}
    \item \textbf{Additivity:} For all vectors $u_1, u_2 \in U$,
    \[ T(u_1 + u_2) = T(u_1) + T(u_2) \]
    (The '+' on the left is addition in $U$; the '+' on the right is addition in $V$).
    \item \textbf{Homogeneity:} For all vectors $u \in U$ and all scalars $c \in \F$,
    \[ T(c u) = c T(u) \]
    (Scalar multiplication on the left is in $U$; on the right, it's in $V$).
\end{enumerate}
These two conditions can be combined into a single condition: $T(c_1 u_1 + c_2 u_2) = c_1 T(u_1) + c_2 T(u_2)$ for all $u_1, u_2 \in U$ and $c_1, c_2 \in \F$. By induction, this extends to any finite linear combination: $T(\sum c_i u_i) = \sum c_i T(u_i)$.
\end{definition}

\begin{example}[Linearity Check]
Let's check the functions from the previous example for linearity, assuming $U=V=\R$ (a vector space over $\F=\R$).
\begin{enumerate}
    \item $f(x) = x$.
        \begin{itemize}
            \item Additivity: $f(x_1 + x_2) = x_1 + x_2$. Also, $f(x_1) + f(x_2) = x_1 + x_2$. They are equal.
            \item Homogeneity: $f(cx) = cx$. Also, $c f(x) = c x$. They are equal.
            \item Conclusion: $f(x)=x$ is a linear transformation.
        \end{itemize}
    \item $g(x) = x^2$.
        \begin{itemize}
            \item Additivity: $g(x_1 + x_2) = (x_1 + x_2)^2 = x_1^2 + 2x_1x_2 + x_2^2$. However, $g(x_1) + g(x_2) = x_1^2 + x_2^2$. These are generally not equal (unless $x_1$ or $x_2$ is 0). For example, $g(1+1) = g(2) = 4$, but $g(1)+g(1) = 1^2+1^2 = 2$. Additivity fails.
            \item Homogeneity: $g(cx) = (cx)^2 = c^2 x^2$. However, $c g(x) = c x^2$. These are generally not equal (unless $c=0, c=1$, or $x=0$). Homogeneity fails.
            \item Conclusion: $g(x)=x^2$ is *not* a linear transformation.
        \end{itemize}
\end{enumerate}
\end{example}

\begin{example}[Matrix Multiplication]
Let $A$ be a fixed $m \times k$ matrix with entries in $\F$. Define a transformation $T: \F^k \to \F^m$ by $T(x) = Ax$, where $x \in \F^k$ is treated as a $k \times 1$ column vector. Is $T$ linear?
\begin{itemize}
    \item Additivity: $T(x+y) = A(x+y) = Ax + Ay = T(x) + T(y)$, using properties of matrix multiplication. Yes.
    \item Homogeneity: $T(cx) = A(cx) = c(Ax) = c T(x)$, using properties of matrix multiplication. Yes.
\end{itemize}
Conclusion: Multiplication by a fixed matrix $A$ defines a linear transformation from $\F^k$ (domain of inputs $x$) to $\F^m$ (codomain of outputs $Ax$). This is a fundamental example, and we will see that, in finite dimensions, all linear transformations can be represented this way.
\end{example}

A simple but important property holds for all linear transformations:

\begin{lemma}
If $T: U \to V$ is a linear transformation, then $T(0_U) = 0_V$. (The zero vector in the domain maps to the zero vector in the codomain).
\end{lemma}
\begin{proof}
We know that $0_U = 0_U + 0_U$ (property of the zero vector in $U$). Applying $T$ to both sides:
\[ T(0_U) = T(0_U + 0_U) \]
By the additivity property of linear transformations:
\[ T(0_U) = T(0_U) + T(0_U) \]
Now, let $w = T(0_U)$. This vector $w$ is an element of the vector space $V$. In $V$, $w$ has an additive inverse, $-w$. Adding $-w$ to both sides of the equation $w = w + w$:
\[ w + (-w) = (w + w) + (-w) \]
Using associativity and the definition of additive inverse in $V$:
\[ 0_V = w + (w + (-w)) \]
\[ 0_V = w + 0_V \]
\[ 0_V = w \]
Substituting back $w = T(0_U)$, we get $T(0_U) = 0_V$.
\end{proof}
\begin{remark}
This lemma provides a quick necessary condition for linearity. If a transformation $T$ does *not* map the zero vector of its domain to the zero vector of its codomain, it cannot be linear. For example, $f(x) = x+1$ is not linear because $f(0) = 1 \neq 0$. However, $T(0_U) = 0_V$ is not sufficient to guarantee linearity (e.g., $g(x)=x^2$ satisfies $g(0)=0$ but is not linear).
\end{remark}

\section{Isomorphisms: Equivalence of Vector Spaces}

We now combine the ideas of bijectivity (one-to-one and onto) and linearity.

\begin{definition}[Isomorphism]
Let $U$ and $V$ be vector spaces over the same field $\F$. A linear transformation $T: U \to V$ is called an \emph{isomorphism} if it is both one-to-one (injective) and onto (surjective).
If an isomorphism exists between $U$ and $V$, we say that $U$ and $V$ are \emph{isomorphic}, denoted $U \cong V$.
\end{definition}

The term "isomorphism" comes from Greek: "iso" meaning "equal" and "morph" meaning "form" or "shape." Isomorphic vector spaces have the same structure from the perspective of vector space operations. They are essentially just different representations of the same underlying abstract structure.

\begin{example}[Flattening Matrices - A Key Isomorphism]
Consider the space $U = \F^{2 \times 2}$ of $2 \times 2$ matrices with entries in $\F$, and the space $V = \F^4$ of 4-component column vectors (or row vectors, the representation isn't crucial here, just the number of components) over $\F$. Define a transformation $T: \F^{2 \times 2} \to \F^4$ by "flattening" the matrix into a vector:
\[ T\left( \mat{a & b \\ c & d} \right) = \begin{pmatrix} a \\ b \\ c \\ d \end{pmatrix} \]
(The original transcript used $(a, b, c, d)$, which is fine too; we'll use column vectors for consistency with $\F^n$). Let's prove that $T$ is an isomorphism.

\textbf{1. Is $T$ linear?}
Let $A = \mat{a_{11} & a_{12} \\ a_{21} & a_{22}}$ and $B = \mat{b_{11} & b_{12} \\ b_{21} & b_{22}}$ be two matrices in $\F^{2 \times 2}$, and let $k \in \F$ be a scalar.
\begin{itemize}
    \item Additivity:
    \begin{align*} T(A+B) &= T\left( \mat{a_{11}+b_{11} & a_{12}+b_{12} \\ a_{21}+b_{21} & a_{22}+b_{22}} \right) \\ &= \mat{a_{11}+b_{11} \\ a_{12}+b_{12} \\ a_{21}+b_{21} \\ a_{22}+b_{22}} \\ &= \mat{a_{11} \\ a_{12} \\ a_{21} \\ a_{22}} + \mat{b_{11} \\ b_{12} \\ b_{21} \\ b_{22}} \\ &= T(A) + T(B) \end{align*}
    This holds.
    \item Homogeneity:
    \begin{align*} T(kA) &= T\left( \mat{ka_{11} & ka_{12} \\ ka_{21} & ka_{22}} \right) \\ &= \mat{ka_{11} \\ ka_{12} \\ ka_{21} \\ ka_{22}} \\ &= k \mat{a_{11} \\ a_{12} \\ a_{21} \\ a_{22}} \\ &= k T(A) \end{align*}
    This also holds.
\end{itemize}
Therefore, $T$ is a linear transformation.

\textbf{2. Is $T$ one-to-one (injective)?}
Suppose $T(A) = T(B)$. This means:
\[ \mat{a_{11} \\ a_{12} \\ a_{21} \\ a_{22}} = \mat{b_{11} \\ b_{12} \\ b_{21} \\ b_{22}} \]
By the definition of vector equality, this implies $a_{11}=b_{11}$, $a_{12}=b_{12}$, $a_{21}=b_{21}$, and $a_{22}=b_{22}$. But this means the matrices $A$ and $B$ have identical entries, so $A = B$.
Thus, $T$ is one-to-one.

\textbf{3. Is $T$ onto (surjective)?}
Let $v = \mat{v_1 \\ v_2 \\ v_3 \\ v_4}$ be an arbitrary vector in the codomain $\F^4$. Can we find a matrix $A \in \F^{2 \times 2}$ such that $T(A) = v$?
Yes, simply choose the matrix $A = \mat{v_1 & v_2 \\ v_3 & v_4}$. By the definition of $T$, we have
\[ T(A) = T\left( \mat{v_1 & v_2 \\ v_3 & v_4} \right) = \mat{v_1 \\ v_2 \\ v_3 \\ v_4} = v \]
Since for any vector $v$ in the codomain, we can construct a matrix $A$ in the domain that maps to it, $T$ is onto.

\textbf{Conclusion:} Since $T$ is linear, one-to-one, and onto, $T$ is an isomorphism. Therefore, the vector space $\F^{2 \times 2}$ is isomorphic to the vector space $\F^4$:
\[ \F^{2 \times 2} \cong \F^4 \]
This confirms our intuition that a $2 \times 2$ matrix just holds 4 independent numbers, much like a 4-component vector.
\end{example}

\section{Properties Preserved by Isomorphisms}

Isomorphisms are powerful because they preserve the essential structure of vector spaces. This means that properties related to vector addition and scalar multiplication behave identically in isomorphic spaces. Two key examples are linear independence and spanning.

\begin{theorem}[Isomorphisms Preserve Linear Independence]
Let $T: U \to V$ be an isomorphism. Let $S = \{u_1, u_2, \dots, u_k\}$ be a subset of $U$. Then $S$ is linearly independent in $U$ if and only if the set of images $T(S) = \{T(u_1), T(u_2), \dots, T(u_k)\}$ is linearly independent in $V$.
\end{theorem}

\begin{proof} We need to prove both directions.

($\Rightarrow$) Assume $S$ is linearly independent in $U$. We want to show $T(S)$ is linearly independent in $V$. We will use proof by contradiction. Suppose $T(S)$ is linearly dependent. By definition of linear dependence, there exist scalars $c_1, c_2, \dots, c_k \in \F$, not all zero, such that:
\[ c_1 T(u_1) + c_2 T(u_2) + \dots + c_k T(u_k) = 0_V \]
Since $T$ is a linear transformation, we can rewrite the left side:
\[ T(c_1 u_1 + c_2 u_2 + \dots + c_k u_k) = 0_V \]
We also know from the lemma that $T(0_U) = 0_V$. So we have:
\[ T(c_1 u_1 + c_2 u_2 + \dots + c_k u_k) = T(0_U) \]
Because $T$ is an isomorphism, it is one-to-one (injective). Therefore, if the images are equal, the inputs must be equal:
\[ c_1 u_1 + c_2 u_2 + \dots + c_k u_k = 0_U \]
However, we assumed that $S = \{u_1, \dots, u_k\}$ is linearly independent. This means the only way their linear combination can be the zero vector is if all scalars are zero ($c_1 = c_2 = \dots = c_k = 0$). This contradicts our starting point for the proof by contradiction, where we assumed that not all $c_i$ were zero.
Therefore, our initial assumption that $T(S)$ is linearly dependent must be false. Hence, $T(S)$ is linearly independent.

($\Leftarrow$) Assume $T(S)$ is linearly independent in $V$. We want to show $S$ is linearly independent in $U$. (This direction uses the linearity and injectivity of $T$ similarly, or one can consider the inverse isomorphism $T^{-1}: V \to U$, which is also linear, and apply the first part of the proof. This is left as an exercise for the reader.)
\end{proof}

\begin{theorem}[Isomorphisms Preserve Spanning Sets]
Let $T: U \to V$ be an isomorphism. Let $S = \{u_1, u_2, \dots, u_k\}$ be a subset of $U$. Then $S$ spans $U$ if and only if the set of images $T(S) = \{T(u_1), T(u_2), \dots, T(u_k)\}$ spans $V$.
\end{theorem}

\begin{proof} Again, we prove both directions.

($\Rightarrow$) Assume $S$ spans $U$. This means every vector $u \in U$ can be written as a linear combination $u = \sum_{i=1}^k a_i u_i$ for some scalars $a_i \in \F$. We want to show that $T(S)$ spans $V$. That is, we need to show that any vector $v \in V$ can be written as a linear combination of the vectors in $T(S)$.
Let $v$ be an arbitrary vector in $V$. Since $T$ is an isomorphism, it is onto (surjective). Therefore, there exists some vector $u \in U$ such that $T(u) = v$.
Since $S$ spans $U$, we can write this $u$ as a linear combination:
\[ u = a_1 u_1 + a_2 u_2 + \dots + a_k u_k \]
for some scalars $a_1, \dots, a_k \in \F$. Now apply $T$ to both sides:
\[ T(u) = T(a_1 u_1 + a_2 u_2 + \dots + a_k u_k) \]
Since $T(u) = v$ and $T$ is linear:
\[ v = a_1 T(u_1) + a_2 T(u_2) + \dots + a_k T(u_k) \]
This shows that $v$ is a linear combination of the vectors in $T(S) = \{T(u_1), \dots, T(u_k)\}$. Since $v$ was an arbitrary vector in $V$, the set $T(S)$ spans $V$.

($\Leftarrow$) Assume $T(S)$ spans $V$. We want to show $S$ spans $U$. (This direction uses the linearity and surjectivity of $T$ similarly, or again, one can consider $T^{-1}$. Left as an exercise.)
\end{proof}

These two theorems have a profound consequence regarding the dimension of vector spaces. Recall that a basis is a linearly independent spanning set.

\begin{corollary}[Isomorphic Spaces Have the Same Dimension]
If $U$ and $V$ are isomorphic finite-dimensional vector spaces ($U \cong V$), then they have the same dimension: $\dimv(U) = \dimv(V)$.
\end{corollary}
\begin{proof}
Let $S = \{u_1, \dots, u_n\}$ be a basis for $U$. By definition, $S$ is linearly independent and spans $U$, and $\dimv(U) = n$.
Let $T: U \to V$ be an isomorphism. Consider the set of images $T(S) = \{T(u_1), \dots, T(u_n)\}$.
\begin{itemize}
    \item By Theorem 4.1, since $S$ is linearly independent, $T(S)$ is linearly independent in $V$.
    \item By Theorem 4.2, since $S$ spans $U$, $T(S)$ spans $V$.
\end{itemize}
Since $T(S)$ is a linearly independent spanning set for $V$, it is a basis for $V$. The number of vectors in the basis $T(S)$ is $n$. Therefore, $\dimv(V) = n$.
We conclude that $\dimv(U) = \dimv(V) = n$.
\end{proof}
\begin{remark}
The converse is also true for finite-dimensional vector spaces over the same field: If $\dimv(U) = \dimv(V)$, then $U \cong V$. We will prove this using coordinate maps. This means that finite-dimensional vector spaces are completely characterized (up to isomorphism) by their dimension.
\end{remark}

\begin{example}[Dimension of Matrix Space]
What is the dimension of the vector space $V = \F^{m \times n}$ (the space of $m \times n$ matrices over $\F$)?
Generalizing Example 3.2, we can define a "flattening" map $T: \F^{m \times n} \to \F^{mn}$ that takes an $m \times n$ matrix and lists its $mn$ entries in some fixed order (e.g., row by row) to form a vector in $\F^{mn}$. Just as in the $2 \times 2$ case, one can prove that this map $T$ is an isomorphism.
\[ \F^{m \times n} \cong \F^{mn} \]
We know that the dimension of the standard vector space $\F^{mn}$ is $mn$. Since the spaces are isomorphic, they must have the same dimension according to Corollary 4.3.
Therefore, $\dimv(\F^{m \times n}) = mn$.
For instance, $\dimv(\F^{2 \times 2}) = 2 \times 2 = 4$, and $\dimv(\F^{3 \times 8}) = 3 \times 8 = 24$.
This result allows us to know, for example, that any basis for $\F^{m \times n}$ must contain exactly $mn$ matrices.
\end{example}

\section{The Coordinate Isomorphism: Making Abstraction Concrete}

We saw that $\F^{m \times n} \cong \F^{mn}$. This suggests a powerful idea: can we always find an isomorphism between any finite-dimensional vector space $V$ (over $\F$) of dimension $n$ and the standard space $\F^n$? Yes! This is achieved through the concept of coordinates.

The key insight is that once we choose a basis for $V$, every vector in $V$ can be uniquely represented as a linear combination of the basis vectors. The coefficients of this combination are precisely the "coordinates" we are looking for.

\begin{definition}[Ordered Basis]
An \emph{ordered basis} for a finite-dimensional vector space $V$ is a basis $S = (s_1, s_2, \dots, s_n)$ where the order of the vectors matters. We use parentheses $( \dots )$ instead of braces $\{ \dots \}$ to emphasize the order.
\end{definition}
Why does order matter? Because we want to map a vector $v \in V$ to a unique vector $(c_1, c_2, \dots, c_n)$ in $\F^n$. The position in the $n$-tuple (first component, second component, etc.) must correspond to a specific basis vector.

\begin{definition}[Coordinate Vector]
Let $V$ be a finite-dimensional vector space over $\F$, and let $S = (s_1, s_2, \dots, s_n)$ be an ordered basis for $V$. For any vector $v \in V$, there exists a \emph{unique} set of scalars $c_1, c_2, \dots, c_n \in \F$ such that
\[ v = c_1 s_1 + c_2 s_2 + \dots + c_n s_n \]
The \emph{coordinate vector} of $v$ relative to the ordered basis $S$, denoted $\coord{v}{S}$, is the vector in $\F^n$ formed by these unique scalars:
\[ \coord{v}{S} \coloneqq \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix} \in \F^n \]
\end{definition}
The uniqueness of the scalars $c_i$ comes directly from the definition of a basis (specifically, from linear independence, which guarantees uniqueness of representation).

\begin{definition}[Coordinate Map]
Let $V$ be an $n$-dimensional vector space over $\F$ with an ordered basis $S = (s_1, \dots, s_n)$. The \emph{coordinate map} (or coordinate representation) relative to $S$ is the function $\psi_S: V \to \F^n$ defined by
\[ \psi_S(v) = \coord{v}{S} \]
This map takes an abstract vector $v \in V$ and produces its concrete coordinate vector in $\F^n$.
\end{definition}

\begin{example}[Coordinates of a Polynomial]
Let $V = P_2(\R)$ be the vector space of polynomials of degree at most 2 with real coefficients. The dimension of $V$ is 3.
\begin{enumerate}
    \item Consider the \emph{standard ordered basis} $S = (1, x, x^2)$. Let $p(x) = 3 + 2x + 1x^2$.
    This polynomial is already written as a linear combination of the basis vectors in $S$. The coefficients are $c_1=3$ (for basis vector 1), $c_2=2$ (for basis vector $x$), and $c_3=1$ (for basis vector $x^2$).
    Therefore, the coordinate vector of $p(x)$ relative to $S$ is:
    \[ \coord{p}{S} = \mat{3 \\ 2 \\ 1} \in \R^3 \]
    \emph{Important Note on Order:} The transcript originally wrote $p(x)=x^2+2x+3$ and obtained $\coord{p}{S}=(3,2,1)$. This implies the basis was ordered $(1, x, x^2)$. If the basis had been ordered $(x^2, x, 1)$, the coordinate vector would be $(1, 2, 3)$. The order must be fixed and respected!

    \item Now consider a \emph{different ordered basis} for $P_2(\R)$, say $T = (1, x+1, (x+1)^2)$.
    Let's verify $T$ is indeed a basis. $(x+1)^2 = x^2+2x+1$. The polynomials are $1, 1+x, 1+2x+x^2$. They have degrees 0, 1, 2, respectively, so they are linearly independent. Since there are 3 of them and $\dim(P_2(\R))=3$, they form a basis.
    Let's find the coordinates of the \emph{same} polynomial $p(x) = x^2 + 2x + 3$ relative to this new basis $T$. We need to find unique scalars $a_1, a_2, a_3$ such that:
    \[ p(x) = a_1 (1) + a_2 (x+1) + a_3 (x+1)^2 \]
    Expanding the right side:
    \[ x^2 + 2x + 3 = a_1 + a_2 x + a_2 + a_3 (x^2 + 2x + 1) \]
    \[ x^2 + 2x + 3 = a_1 + a_2 x + a_2 + a_3 x^2 + 2a_3 x + a_3 \]
    Collect terms by powers of $x$:
    \[ (1)x^2 + (2)x + (3) = (a_3)x^2 + (a_2 + 2a_3)x + (a_1 + a_2 + a_3)1 \]
    Equating the coefficients of corresponding powers of $x$ (this relies on the linear independence of $\{1, x, x^2\}$):
    \begin{align*} \text{coeff of } x^2: \quad & a_3 = 1 \\ \text{coeff of } x^1: \quad & a_2 + 2a_3 = 2 \\ \text{coeff of } x^0: \quad & a_1 + a_2 + a_3 = 3 \end{align*}
    This is a system of linear equations for $a_1, a_2, a_3$. We can solve it easily by back-substitution:
    \begin{itemize}
        \item From eq 1: $a_3 = 1$.
        \item Substitute into eq 2: $a_2 + 2(1) = 2 \implies a_2 = 0$.
        \item Substitute $a_2, a_3$ into eq 3: $a_1 + 0 + 1 = 3 \implies a_1 = 2$.
    \end{itemize}
    So, the unique scalars are $a_1=2, a_2=0, a_3=1$.
    Therefore, the coordinate vector of $p(x)$ relative to $T$ is:
    \[ \coord{p}{T} = \mat{2 \\ 0 \\ 1} \in \R^3 \]
    Notice how the *same* abstract vector $p(x)$ has *different* coordinate representations depending on the chosen ordered basis. $\coord{p}{S} \neq \coord{p}{T}$.
\end{enumerate}
\end{example}

The central result connecting abstract finite-dimensional spaces to $\F^n$ is the following theorem:

\begin{theorem}[The Coordinate Map is an Isomorphism]
Let $V$ be an $n$-dimensional vector space over $\F$, and let $S = (s_1, \dots, s_n)$ be any ordered basis for $V$. The coordinate map $\psi_S: V \to \F^n$ defined by $\psi_S(v) = \coord{v}{S}$ is an isomorphism.
\end{theorem}

\begin{proof}
We need to show that $\psi_S$ is linear, one-to-one, and onto.

\textbf{1. Linearity:} (This part was mentioned as needed but its proof was deferred in the transcript).
Let $v, w \in V$ and $k \in \F$. We need to show $\psi_S(v+w) = \psi_S(v) + \psi_S(w)$ and $\psi_S(kv) = k \psi_S(v)$.
Let $\coord{v}{S} = (c_1, \dots, c_n)^T$ and $\coord{w}{S} = (d_1, \dots, d_n)^T$. This means:
$v = \sum c_i s_i$ and $w = \sum d_i s_i$.
Then $v+w = \sum c_i s_i + \sum d_i s_i = \sum (c_i + d_i) s_i$.
By definition of coordinates, the unique coefficients for $v+w$ relative to basis $S$ are $(c_i+d_i)$. So:
\[ \psi_S(v+w) = \coord{v+w}{S} = \begin{pmatrix} c_1+d_1 \\ \vdots \\ c_n+d_n \end{pmatrix} = \begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix} + \begin{pmatrix} d_1 \\ \vdots \\ d_n \end{pmatrix} = \coord{v}{S} + \coord{w}{S} = \psi_S(v) + \psi_S(w) \]
Also, $kv = k(\sum c_i s_i) = \sum (kc_i) s_i$. The unique coefficients for $kv$ are $(kc_i)$. So:
\[ \psi_S(kv) = \coord{kv}{S} = \begin{pmatrix} kc_1 \\ \vdots \\ kc_n \end{pmatrix} = k \begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix} = k \coord{v}{S} = k \psi_S(v) \]
Thus, $\psi_S$ is linear.

\textbf{2. One-to-one (Injective):}
Suppose $\psi_S(v) = \psi_S(w)$. This means $\coord{v}{S} = \coord{w}{S}$. Let $\coord{v}{S} = (c_1, \dots, c_n)^T$ and $\coord{w}{S} = (d_1, \dots, d_n)^T$. Then $c_i = d_i$ for all $i=1, \dots, n$.
By definition of coordinates, $v = \sum c_i s_i$ and $w = \sum d_i s_i$. Since $c_i = d_i$ for all $i$, we have $v = w$.
Therefore, $\psi_S$ is one-to-one.
(Alternative using linearity, as in the transcript: Suppose $\psi_S(v) = \psi_S(w)$. Then $\psi_S(v) - \psi_S(w) = 0_{\F^n}$. Since $\psi_S$ is linear (from step 1), $\psi_S(v-w) = 0_{\F^n}$. This means the coordinate vector of $v-w$ relative to $S$ is the zero vector. Let $v-w = \sum a_i s_i$. Then $a_i=0$ for all $i$. Thus $v-w = \sum 0 \cdot s_i = 0_V$, which implies $v=w$.)

\textbf{3. Onto (Surjective):}
Let $x = (x_1, \dots, x_n)^T$ be an arbitrary vector in the codomain $\F^n$. Can we find a vector $v \in V$ such that $\psi_S(v) = x$?
Consider the vector $v$ constructed using the components of $x$ as coefficients for the basis $S$:
\[ v = x_1 s_1 + x_2 s_2 + \dots + x_n s_n \]
This vector $v$ is certainly in $V$ (since $V$ is closed under linear combinations). By the definition of the coordinate map, the coordinates of this specific $v$ relative to $S$ are precisely $(x_1, \dots, x_n)^T$.
\[ \psi_S(v) = \coord{v}{S} = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} = x \]
Since for any $x \in \F^n$, we found a $v \in V$ that maps to it, $\psi_S$ is onto.

\textbf{Conclusion:} Since $\psi_S$ is linear, one-to-one, and onto, it is an isomorphism.
\end{proof}

This theorem is fundamental. It tells us that any $n$-dimensional vector space $V$ over $\F$ is isomorphic to $\F^n$. This means we can study abstract $n$-dimensional spaces by studying the concrete space $\F^n$, translating questions about vectors in $V$ to questions about column vectors in $\F^n$ via the coordinate map, solving them there (often using familiar matrix techniques), and then translating the answers back if needed. This connection justifies the extensive focus on $\R^n$ and matrices in introductory linear algebra â€“ they provide the tools to understand all finite-dimensional vector spaces.

\end{document}